diff --git a/graph.py b/graph.py
index 8afdb49..abe2e6b 100644
--- a/graph.py
+++ b/graph.py
@@ -132,7 +132,7 @@ class DeployGraph(object):
                 try:
                     graph, _params_dict, _torch_out = torch.onnx.utils._model_to_graph(module, dummy_input, propagate=True, _retain_param_name=True)
                 except TypeError:
-                    graph, _params_dict, _torch_out = torch.onnx.utils._model_to_graph(module, dummy_input, _retain_param_name=True)
+                    graph, _params_dict, _torch_out = torch.onnx.utils._model_to_graph(module, dummy_input)
         input_dict = {}
         output_dict = {}
         self.non_unique_names_dict = {}
diff --git a/quant/pact.py b/quant/pact.py
index 2914620..f056658 100644
--- a/quant/pact.py
+++ b/quant/pact.py
@@ -78,6 +78,18 @@ def pact_quantize_deploy(W, eps, clip):
     W = W.clamp(0, clip)
     return W
 
+# PACT quantization for inference of bias
+def pact_quantize_inference_bias(b, eps, clip):
+    b_quant = b.clone().detach()
+    b_quant.data[:] = (b_quant.data[:] / eps).floor()*eps
+    b_quant.clamp_(0, clip.item())
+    return b_quant
+
+def pact_quantize_deploy_bias(b, eps, clip):
+    b = (b / eps).floor()*eps
+    b = b.clamp(0, clip)
+    return b
+
 # PACT signed quantization for inference (workaround for pact_quantize_signed not functional in inference)
 def pact_quantize_signed_inference(W, eps, clip):
     return pact_quantize_asymm_inference(W, torch.as_tensor(eps), torch.as_tensor(clip), torch.as_tensor(clip))
@@ -100,6 +112,32 @@ def pact_quantize_asymm_inference(W, eps, alpha, beta, train_loop=True, train_lo
     W_quant.clamp_(-alpha.item(), beta.item() + eps.item())
     return W_quant
 
+# PACT signed quantization for inference (workaround for pact_quantize_signed not functional in inference)
+def pact_quantize_signed_inference_bias(b, eps, clip):
+    return pact_quantize_asymm_inference_bias(b, torch.as_tensor(eps), torch.as_tensor(clip), torch.as_tensor(clip))
+
+# PACT asymmetric quantization for inference (workaround for pact_quantize_asymm not functional in inference)
+def pact_quantize_asymm_inference_bias(b, eps, alpha, beta, train_loop=True, train_loop_oldprec_b=None):
+    # for numerical reasons, b_quant should be put at a "small_eps" of its "pure" value to enable
+    # running againt the weights through pact_quantize_asymm_inference (the torch.floor function
+    # won't return the correct value otherwise)
+    # we choose small_eps = eps/2
+    try:
+        if not train_loop and train_loop_oldprec_b is not None:
+            b_quant = b.clone().detach()
+            b_quant.data[:] = (b_quant.data[:] / train_loop_oldprec_b).floor()*train_loop_oldprec_b + eps*0.5
+        else:
+            b_quant = b.clone().detach() + eps*0.5
+        b_quant.data[:] = (b_quant.data[:] / eps).floor()*eps
+        # alpha, beta are also represented with quantized numbers
+        alpha = torch.ceil(alpha/eps)*eps
+        beta  = torch.floor(beta/eps)*eps
+        b_quant.clamp_(-alpha.item(), beta.item() + eps.item())
+    except:
+        b_quant = b
+    return b_quant
+
+
 # DEPRECATED
 def pact_pwl(x, eps, alpha, beta, q0=0):
     beta = beta.abs()
@@ -370,7 +408,7 @@ class PACT_Act(torch.nn.Module):
             self.D = D
         else:
             self.D = min(D, 2.0**(32-1-(self.precision.get_bits())))
-
+        self.D = torch.round(self.D)    # try to avoid errors of rounding
     def get_output_eps(self, eps_in):
         r"""Get the output quantum (:math:`\varepsilon`) given the input one.
 
@@ -575,12 +613,12 @@ class PACT_IntegerAct(torch.nn.Module):
         # self.eps_out   = self.alpha.item()/(2.0**(self.precision.get_bits())-1)
         self.alpha_out = 2.0**(self.precision.get_bits())-1
         # D is selected as a power-of-two
-        D = 2.0**torch.ceil(torch.log2(self.requantization_factor * self.eps_out / self.eps_in))
+        D = 2.0**torch.ceil(torch.log2(self.requantization_factor * self.eps_out / self.eps_in)) # can be not integer for some reason
         if not limit_at_32_bits:
             self.D = D
         else:
             self.D = min(D, 2.0**(32-(self.precision.get_bits())))
-
+        self.D = torch.round(self.D) #avoid error 65535.9999.. roundedn to 65535.99 in first Relu
     def get_output_eps(self, eps_in):
         r"""Get the output quantum (:math:`\varepsilon`) given the input one.
 
@@ -612,7 +650,6 @@ class PACT_IntegerAct(torch.nn.Module):
         :rtype:  :py:class:`torch.Tensor`
 
         """
-
         x_rq = pact_integer_requantize(x, self.eps_in, self.eps_out, self.D)
         return x_rq.clamp(0, self.alpha_out)
 
@@ -946,11 +983,14 @@ class PACT_Conv2d(torch.nn.Conv2d):
         kernel_size,
         quantize_x=False,
         quantize_W=True,
+        quantize_b=True,
         quantize_y=False,
         W_precision=None,
+        b_precision=None,
         x_precision=None,
         alpha=1.,
         quant_asymm=True,
+        quant_asymm_b=True,
         **kwargs
     ):
         r"""Constructor. Supports all arguments supported by :py:class:`torch.nn.Conv2d` plus additional ones.
@@ -972,6 +1012,11 @@ class PACT_Conv2d(torch.nn.Conv2d):
             self.W_precision = Precision()
         else:
             self.W_precision = Precision(bits=W_precision.get_bits())
+        if b_precision is None:
+            self.b_precision = Precision()
+        else:
+            self.b_precision = Precision(bits=b_precision.get_bits())
+        
         if x_precision is None:
             self.x_precision = Precision()
         else:
@@ -979,25 +1024,34 @@ class PACT_Conv2d(torch.nn.Conv2d):
 
         self.quantize_x = quantize_x
         self.quantize_W = quantize_W
+        self.quantize_b = quantize_b
 
         super(PACT_Conv2d, self).__init__(in_channels, out_channels, kernel_size, **kwargs)
         device = 'cuda' if torch.cuda.is_available() else 'cpu'
         self.W_alpha = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
         if quant_asymm:
             self.W_beta  = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
+        
+        self.b_alpha = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
+        if quant_asymm_b:
+            self.b_beta  = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
 
         self.x_alpha = torch.nn.Parameter(torch.Tensor((2.0,)).to(device))
         self.weight.data.uniform_(-1., 1.)
+        if self.bias is not None:
+            self.bias.data.uniform_(0., 0.)
 
         self.quant_asymm = quant_asymm
-
+        self.quant_asymm_b = quant_asymm_b
         self.train_loop = True
         self.deployment = False
         self.train_loop_oldprec = None
-
+        self.train_loop_oldprec_b = None
         self.padding_value = 0
         self.hardened = False
         self.integerized = False
+        self.hardened_b = False
+        self.integerized_b = False
         self.eps_out_static = None
 
     def reset_alpha_weights(self, use_method='max', nb_std=5., verbose=False, dyn_range_bins=1024, dyn_range_cutoff=0., **kwargs):
@@ -1073,6 +1127,7 @@ class PACT_Conv2d(torch.nn.Conv2d):
             else:
                 eps = 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1)
                 self.weight.data = pact_quantize_signed_inference(self.weight, eps, self.W_alpha) / eps
+            self.weight.data = self.weight.data.round()
             self.integerized = True
 
     def prune_weights(self, threshold=0.1, eps=2**-9.):
@@ -1096,6 +1151,87 @@ class PACT_Conv2d(torch.nn.Conv2d):
         logging.info("[Pruning] Pruned %d" % np.count_nonzero(wc < eps))
         return np.count_nonzero(wc < eps)
 
+    def reset_alpha_bias(self, use_method='max', nb_std=5., verbose=False, dyn_range_bins=1024, dyn_range_cutoff=0., **kwargs):
+        r"""Resets :math:`\alpha` and :math:`\beta` parameters for bias.
+
+        """
+        try:
+            if not self.quant_asymm_b:
+                self.b_alpha.data[0] = self.bias.data.abs().max()
+            elif use_method=='max':
+                self.b_alpha.data[0] = -self.bias.data.min()
+                self.b_beta.data [0] =  self.bias.data.max()
+            elif use_method=='std':
+                self.b_alpha.data[0] = -self.bias.data.mean() + nb_std*self.bias.data.std()
+                self.b_beta.data[0]  =  self.bias.data.mean() + nb_std*self.bias.data.std()
+            elif use_method=='dyn_range':
+                import scipy.stats
+                alpha_n = self.bias.data.min()
+                beta    = self.bias.data.max()
+                x = np.linspace(alpha_n.cpu().detach().numpy(), beta.cpu().detach().numpy(), dyn_range_bins)
+                res = scipy.stats.cumfreq(self.bias.data.cpu().detach().numpy(), dyn_range_bins, defaultreallimits=(alpha_n.cpu().detach().numpy(), beta.cpu().detach().numpy()))
+                yh = res.cumcount / res.cumcount[-1]
+                if not (yh<dyn_range_cutoff).any() and not (yh>1-dyn_range_cutoff).any():
+                    self.b_alpha.data[0] = torch.as_tensor(-x.min(), device=self.b_alpha.data.device)
+                    self.b_beta.data[0]  = torch.as_tensor(x.max(), device=self.b_beta.data.device)
+                elif not (yh<dyn_range_cutoff).any():
+                    self.b_alpha.data[0] = torch.as_tensor(-x[yh>1-dyn_range_cutoff].min(), device=self.b_alpha.data.device)
+                    self.b_beta.data[0]  = torch.as_tensor(x.max(), device=self.b_beta.data.device)
+                elif not (yh>1-dyn_range_cutoff).any():
+                    self.B_alpha.data[0] = torch.as_tensor(-x.min(), device=self.b_alpha.data.device)
+                    self.b_beta.data[0]  = torch.as_tensor(x[yh<dyn_range_cutoff].max(), device=self.b_beta.data.device)
+                else:
+                    self.b_alpha.data[0] = torch.as_tensor(-x[yh>1-dyn_range_cutoff].min(), device=self.b_alpha.data.device)
+                    self.b_beta.data[0]  = torch.as_tensor(x[yh<dyn_range_cutoff].max(), device=self.b_beta.data.device)
+                if self.b_alpha < 0:
+                    self.b_alpha.data[:] = -self.b_alpha.data[:]
+                if self.b_beta < 0:
+                    self.b_beta.data[:] = -self.b_beta.data[:]
+                assert (self.b_alpha >= 0).all()
+                assert (self.b_beta >= 0).all()
+
+            if verbose:
+                logging.info("[Quant] b_alpha = %.5f vs b_min = %.5f" % (self.b_alpha.data[0], self.bias.min()))
+                logging.info("[Quant] B_beta  = %.5f vs b_max = %.5f" % (self.b_beta.data[0], self.bias.max()))
+        except:
+            pass
+    def harden_bias(self):
+        r"""Replaces the current value of bias tensors (full-precision, quantized on forward-prop) with the quantized value.
+
+        """
+        try:
+            if not self.hardened_b:
+                # here, clipping parameters are also quantized in order to cope with the PACT variant utilized here.
+                # in this way, the ID version will be able to use only an integer displacement or none at all if
+                # symmetric bias are used
+                if self.quant_asymm_b:
+                    eps = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                    self.bias.data = pact_quantize_asymm_inference_bias(self.bias, eps, torch.ceil(self.b_alpha/eps)*eps, torch.floor(self.b_beta/eps)*eps, train_loop=False, train_loop_oldprec_b=self.train_loop_oldprec_b)
+                    self.eps_static_b = eps
+                else: 
+                    eps = (2*self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                    self.bias.data = pact_quantize_signed_inference_bias(self.bias, eps, self.b_alpha)
+                self.hardened_b = True
+        except:
+            pass
+    def integerize_bias(self, **kwargs):
+        r"""Replaces the current value of bias tensors with the integer bias (i.e., the bias's quantized image).
+
+        """
+        try:
+            if not self.integerized_b:
+                if self.quant_asymm_b:
+                    eps = self.eps_static_b
+                    self.bias.data = self.bias.data/self.eps_static_b
+                else:
+                    eps = 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1)
+                    self.bias.data = pact_quantize_signed_inference_bias(self.bias, eps, self.b_alpha) / eps
+                self.bias.data = self.bias.data.round()
+                self.integerized_b = True
+        except:
+            pass
+
+
     def get_output_eps(self, eps_in):
         r"""Get the output quantum (:math:`\varepsilon`) given the input one.
 
@@ -1114,6 +1250,24 @@ class PACT_Conv2d(torch.nn.Conv2d):
             self.eps_out_static = eps_W * eps_in
         return self.eps_out_static
 
+    def get_output_eps_b(self, eps_in):
+        r"""Get the output quantum (:math:`\varepsilon`) given the input one.
+
+        :param eps_in: input quantum :math:`\varepsilon_{in}`.
+        :type  eps_in: :py:class:`torch.Tensor`
+        :return: output quantum :math:`\varepsilon_{out}`.
+        :rtype:  :py:class:`torch.Tensor`
+
+        """
+
+        if self.eps_out_static_b is None:
+            if self.quant_asymm_b:
+                eps_b = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+            else:
+                eps_b = 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1)
+            self.eps_out_static_b = eps_b * eps_in
+        return self.eps_out_static_b
+
     def forward(self, input):
         r"""Forward-prop function for PACT-quantized 2d-convolution.
 
@@ -1131,14 +1285,28 @@ class PACT_Conv2d(torch.nn.Conv2d):
                 W_quant = pact_quantize_asymm(self.weight, (self.W_beta+self.W_alpha)/(2.0**(self.W_precision.get_bits())-1), self.W_alpha, self.W_beta)
             else:
                 W_quant = pact_quantize_signed(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
-        elif self.quantize_W and not self.deployment:
-            if self.quant_asymm:
-                eps = (self.W_beta+self.W_alpha)/(2.0**(self.W_precision.get_bits())-1)
-                W_quant = pact_quantize_asymm_inference(self.weight, eps, torch.ceil(self.W_alpha/eps)*eps, torch.floor(self.W_beta/eps)*eps, train_loop=self.train_loop, train_loop_oldprec=self.train_loop_oldprec)
+        if self.training and self.quantize_b and not self.deployment:
+            if self.quant_asymm_b:
+                b_quant = pact_quantize_asymm(self.bias, (self.b_beta + self.b_alpha) / (2.0 ** (self.b_precision.get_bits()) - 1), self.b_alpha, self.b_beta)
             else:
-                W_quant = pact_quantize_signed_inference(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
+                b_quant = pact_quantize_signed(self.bias, 2 * self.b_alpha / (2.0 ** (self.b_precision.get_bits()) - 1), self.b_alpha)
+
+        elif (self.quantize_W or self.quantize_b) and not self.deployment:
+            if self.quantize_W:
+                if self.quant_asymm:
+                    eps = (self.W_beta+self.W_alpha)/(2.0**(self.W_precision.get_bits())-1)
+                    W_quant = pact_quantize_asymm_inference(self.weight, eps, torch.ceil(self.W_alpha/eps)*eps, torch.floor(self.W_beta/eps)*eps, train_loop=self.train_loop, train_loop_oldprec=self.train_loop_oldprec)
+                else:
+                    W_quant = pact_quantize_signed_inference(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
+            if self.quantize_b:
+                if self.quant_asymm_b:
+                    eps = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                    b_quant = pact_quantize_asymm_inference_bias(self.bias, eps, torch.ceil(self.b_alpha/eps)*eps, torch.floor(self.b_beta/eps)*eps, train_loop=self.train_loop, train_loop_oldprec_b=self.train_loop_oldprec_b)
+                else:
+                    b_quant = pact_quantize_signed_inference_bias(self.bias, 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1), self.b_alpha)
         else:
             W_quant = self.weight
+            b_quant = self.bias
         # if input bias is present, padding should be performed using the input bias as padding value instead of 0
         if self.deployment and self.padding is not None and self.bias is not None:
             if type(self.padding) is not tuple and type(self.padding) is not list:
@@ -1151,7 +1319,7 @@ class PACT_Conv2d(torch.nn.Conv2d):
         y = torch.nn.functional.conv2d(
             x_quant,
             W_quant,
-            self.bias, # typically nil
+            b_quant, # typically nil
             self.stride,
             self.padding if not self.deployment or self.bias is None else 0,
             self.dilation,
@@ -1178,9 +1346,11 @@ class PACT_Conv1d(torch.nn.Conv1d):
         quantize_x=False,
         quantize_W=True,
         W_precision=None,
+        b_precision=None,
         x_precision=None,
         alpha=1.,
         quant_asymm=True,
+        quant_asymm_b=True,
         **kwargs
     ):
         r"""Constructor. Supports all arguments supported by :py:class:`torch.nn.Conv2d` plus additional ones.
@@ -1207,26 +1377,39 @@ class PACT_Conv1d(torch.nn.Conv1d):
             self.x_precision = Precision()
         else:
             self.x_precision = Precision(bits=x_precision.get_bits())
+        
+        if b_precision is None:
+            self.b_precision = Precision()
+        else:
+            self.b_precision = Precision(bits=b_precision.get_bits())
 
         self.quantize_x = quantize_x
         self.quantize_W = quantize_W
+        self.quantize_b = quantize_b
+        
         super(PACT_Conv1d, self).__init__(in_channels, out_channels, kernel_size, **kwargs)
         device = 'cuda' if torch.cuda.is_available() else 'cpu'
         self.W_alpha = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
         if quant_asymm:
             self.W_beta  = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
-
+        self.b_alpha = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))    
+        if quant_asymm_b:
+            self.b_beta = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
         self.x_alpha = torch.nn.Parameter(torch.Tensor((2.0,)).to(device))
         self.weight.data.uniform_(-1., 1.)
+        if self.bias is not None:
+            self.bias.data.uniform_(0., 0.)
 
         # FIXME to implement: fix alpha,beta scaling factors for "beautiful" quantization in INT4,6,8
 
         self.quant_asymm = quant_asymm
-
+        self.quant_asymm_b = quant_asymm_b
         self.train_loop = True
         self.deployment = False
         self.train_loop_oldprec = None
         self.hardened = False
+        self.train_loop_oldprec_b = None
+        self.hardened_b = False
 
     def reset_alpha_weights(self, use_max=True, nb_std=5., verbose=False, **kwargs):
         r"""Resets :math:`\alpha` and :math:`\beta` parameters for weights.
@@ -1276,6 +1459,7 @@ class PACT_Conv1d(torch.nn.Conv1d):
         else:
             eps = 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1)
             self.weight.data = pact_quantize_signed_inference(self.weight, eps, self.W_alpha) / eps
+            self.weight.data = self.weight.data.round()
 
     def prune_weights(self, threshold=0.1, eps=2**-9.):
         # logging.info("[Pruning] tau=%.1e", threshold)
@@ -1287,6 +1471,56 @@ class PACT_Conv1d(torch.nn.Conv1d):
         # return np.count_nonzero(wc < eps)
         return 0
 
+    def reset_alpha_bias(self, use_max=True, nb_std=5., verbose=False, **kwargs):
+        r"""Resets :math:`\alpha` and :math:`\beta` parameters for bias.
+
+        """
+
+        if not self.quant_asymm_b:
+            self.b_alpha.data[0] = self.bias.data.abs().max()
+        elif use_max:
+            self.b_alpha.data[0] = -self.bias.data.min()
+            self.b_beta.data [0] =  self.bias.data.max()
+        else:
+            self.b_alpha.data[0] = -self.bias.data.mean() + nb_std*self.bias.data.std()
+            self.b_beta.data[0]  =  self.bias.data.mean() + nb_std*self.bias.data.std()
+        if verbose:
+            logging.info("[Quant] b_alpha = %.5f" % self.b_alpha.data[0])
+            logging.info("[Quant] b_beta  = %.5f" % self.b_beta.data[0])
+
+
+    def harden_bias(self):
+        r"""Replaces the current value of bias tensors (full-precision, quantized on forward-prop) with the quantized value.
+
+        """
+
+        if not self.hardened_b:
+            # here, clipping parameters are also quantized in order to cope with the PACT variant utilized here.
+            # in this way, the ID version will be able to use only an integer displacement or none at all if
+            # symmetric bias are used
+            if self.quant_asymm_b:
+                self.reset_alpha_bias()
+                eps = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                self.bias.data = pact_quantize_asymm_inference_bias(self.bias, eps, torch.ceil(self.b_alpha/eps)*eps, torch.floor(self.b_beta/eps)*eps, train_loop=False, train_loop_oldprec_b=self.train_loop_oldprec_b)
+                self.reset_alpha_bias()
+            else: 
+                eps = (2*self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                self.bias.data = pact_quantize_signed_inference_bias(self.bias, eps, self.b_alpha)
+            self.hardened_b = True
+
+    def integerize_bias(self, **kwargs):
+        r"""Replaces the current value of bias tensors with the integer bias (i.e., the bias's quantized image).
+
+        """
+
+        if self.quant_asymm_b:
+            eps = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+            self.bias.data = pact_quantize_asymm_inference_bias(self.bias, eps, self.b_alpha, self.b_beta, train_loop=False) / eps
+        else:
+            eps = 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1)
+            self.bias.data = pact_quantize_signed_inference_bias(self.bias, eps, self.b_alpha) / eps
+            self.bias.data = self.bias.data.round()
+
     def get_output_eps(self, eps_in):
         r"""Get the output quantum (:math:`\varepsilon`) given the input one.
 
@@ -1302,6 +1536,21 @@ class PACT_Conv1d(torch.nn.Conv1d):
             eps_W = 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1)
         return eps_W * eps_in
 
+    def get_output_eps_b(self, eps_in):
+        r"""Get the output quantum (:math:`\varepsilon`) given the input one.
+
+        :param eps_in: input quantum :math:`\varepsilon_{in}`.
+        :type  eps_in: :py:class:`torch.Tensor`
+        :return: output quantum :math:`\varepsilon_{out}`.
+        :rtype:  :py:class:`torch.Tensor`
+
+        """
+        if self.quant_asymm_b:
+            eps_b = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+        else:
+            eps_b = 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1)
+        return eps_b * eps_in
+
     def forward(self, input):
         r"""Forward-prop function for PACT-quantized 1d-convolution.
 
@@ -1322,12 +1571,27 @@ class PACT_Conv1d(torch.nn.Conv1d):
                 W_quant = pact_quantize_signed_inference(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
         else:
             W_quant = self.weight
+
+        if self.training and self.quantize_b and not self.deployment:
+            if self.quant_asymm_b:
+                b_quant = pact_quantize_asymm(self.bias, (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1), self.b_alpha, self.b_beta)
+            else:
+                b_quant = pact_quantize_signed(self.bias, 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1), self.b_alpha)
+        elif self.quantize_b and not self.deployment:
+            if self.quant_asymm_b:
+                b_quant = pact_quantize_asymm_inference_bias(self.bias, (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1), self.b_alpha, self.b_beta, train_loop=self.train_loop)
+            else:
+                b_quant = pact_quantize_signed_inference_bias(self.bias, 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1), self.b_alpha)
+        else:
+            b_quant = self.bias
+
+    
         if self.padding_mode == 'circular':
             expanded_padding = ((self.padding[0] + 1) // 2, self.padding[0] // 2)
             return torch.nn.functional.conv1d(torch.nn.functional.pad(input, expanded_padding, mode='circular'),
-                            W_quant, self.bias, self.stride,
+                            W_quant, b_quant, self.stride,
                             _single(0), self.dilation, self.groups)
-        return torch.nn.functional.conv1d(input, W_quant, self.bias, self.stride,
+        return torch.nn.functional.conv1d(input, W_quant, b_quant, self.stride,
                         self.padding, self.dilation, self.groups)
 
 class PACT_Linear(torch.nn.Linear):
@@ -1344,10 +1608,13 @@ class PACT_Linear(torch.nn.Linear):
         out_features,
         quantize_x=False,
         quantize_W=True,
+        quantize_b=True,
         W_precision=None,
+        b_precision=None,
         x_precision=None,
         alpha=1.,
         quant_asymm=True,
+        quant_asymm_b=True,
         quant_pc=False,
         **kwargs
     ):
@@ -1355,6 +1622,11 @@ class PACT_Linear(torch.nn.Linear):
             self.W_precision = Precision()
         else:
             self.W_precision = Precision(bits=W_precision.get_bits())
+        if b_precision is None:
+            self.b_precision = Precision()
+        else:
+            self.b_precision = Precision(bits=b_precision.get_bits())
+
         if x_precision is None:
             self.x_precision = Precision()
         else:
@@ -1362,21 +1634,30 @@ class PACT_Linear(torch.nn.Linear):
 
         self.quantize_x = quantize_x
         self.quantize_W = quantize_W
+        self.quantize_b = quantize_b
         super(PACT_Linear, self).__init__(in_features, out_features, **kwargs)
         device = 'cuda' if torch.cuda.is_available() else 'cpu'
         self.W_alpha = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
         if quant_asymm:
             self.W_beta  = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
+        self.b_alpha = torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
+        if quant_asymm_b:
+            self.b_beta  =  torch.nn.Parameter(torch.Tensor((alpha,)).to(device))
         self.x_alpha = torch.nn.Parameter(torch.Tensor((2.0,)).to(device))
         self.weight.data.uniform_(-1., 1.)
-
+        if self.bias is not None:
+            self.bias.data.uniform_(0,0) #check
+        
         self.quant_asymm = quant_asymm
+        self.quant_asymm_b = quant_asymm_b
         self.quant_pc    = quant_pc
         
         self.train_loop = True
         self.deployment = False
         self.train_loop_oldprec = None
         self.hardened = False
+        self.train_loop_oldprec_b = None
+        self.hardened_b = False
 
     def reset_alpha_weights(self, use_max=True, nb_std=5., verbose=False, **kwargs):
         r"""Resets :math:`\alpha` and :math:`\beta` parameters for weights.
@@ -1425,6 +1706,7 @@ class PACT_Linear(torch.nn.Linear):
         else:
             eps = 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1)
             self.weight.data = pact_quantize_signed_inference(self.weight, eps, self.W_alpha) / eps
+            self.weight.data = self.weight.data.round()
 
     def prune_weights(self, threshold=0.1, eps=2**-9.):
         r"""Prunes the weights of the layer.
@@ -1447,6 +1729,55 @@ class PACT_Linear(torch.nn.Linear):
         logging.info("[Pruning] Pruned %d" % np.count_nonzero(wc < eps))
         return np.count_nonzero(wc < eps)
 
+    def reset_alpha_bias(self, use_max=True, nb_std=5., verbose=False, **kwargs):
+        r"""Resets :math:`\alpha` and :math:`\beta` parameters for bias.
+
+        """
+
+        if not self.quant_asymm_b:
+            self.b_alpha.data[0] = self.bias.data.abs().max()
+        elif use_max:
+            self.b_alpha.data[0] = -self.bias.data.min()
+            self.b_beta.data [0] =  self.bias.data.max()
+        else:
+            self.b_alpha.data[0] = -self.bias.data.mean() + nb_std*self.bias.data.std()
+            self.b_beta.data[0]  =  self.bias.data.mean() + nb_std*self.bias.data.std()
+        if verbose:
+            logging.info("[Quant] b_alpha = %.5f" % self.b_alpha.data[0])
+            logging.info("[Quant] b_beta  = %.5f" % self.b_beta.data[0])
+
+    def harden_bias(self):
+        r"""Replaces the current value of bias tensors (full-precision, quantized on forward-prop) with the quantized value.
+
+        """
+
+        if not self.hardened_b:
+            # here, clipping parameters are also quantized in order to cope with the PACT variant utilized here.
+            # in this way, the ID version will be able to use only an integer displacement or none at all if
+            # symmetric bias are used
+            if self.quant_asymm_b:
+                self.reset_alpha_bias()
+                eps = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                self.bias.data = pact_quantize_asymm_inference_bias(self.bias, eps, torch.ceil(self.b_alpha/eps)*eps, torch.floor(self.b_beta/eps)*eps, train_loop=False, train_loop_oldprec_b=self.train_loop_oldprec_b)
+                self.reset_alpha_bias()
+            else: 
+                eps = (2*self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+                self.bias.data = pact_quantize_signed_inference_bias(self.bias, eps, self.b_alpha)
+            self.hardened_b = True
+
+    def integerize_bias(self, **kwargs):
+        r"""Replaces the current value of bias tensors with the integer bias (i.e., the bias's quantized image).
+
+        """
+
+        if self.quant_asymm_b:
+            eps = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+            self.bias.data = pact_quantize_asymm_inference_bias(self.bias, eps, self.b_alpha, self.b_beta, train_loop=False) / eps
+        else:
+            eps = 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1)
+            self.bias.data = pact_quantize_signed_inference_bias(self.bias, eps, self.b_alpha) / eps
+            self.bias.data = self.bias.data.round()
+
     def get_output_eps(self, eps_in):
         r"""Get the output quantum (:math:`\varepsilon`) given the input one.
 
@@ -1463,6 +1794,23 @@ class PACT_Linear(torch.nn.Linear):
             eps_W = 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1)
         return eps_W * eps_in
 
+    def get_output_eps_b(self, eps_in):
+        r"""Get the output quantum (:math:`\varepsilon`) given the input one.
+
+        :param eps_in: input quantum :math:`\varepsilon_{in}`.
+        :type  eps_in: :py:class:`torch.Tensor`
+        :return: output quantum :math:`\varepsilon_{out}`.
+        :rtype:  :py:class:`torch.Tensor`
+
+        """
+
+        if self.quant_asymm_b:
+            eps_b = (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1)
+        else:
+            eps_b = 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1)
+        return eps_b * eps_in
+
+
     def forward(self, input):
         r"""Forward-prop function for PACT-quantized linear layer.
 
@@ -1480,14 +1828,29 @@ class PACT_Linear(torch.nn.Linear):
                 W_quant = pact_quantize_asymm(self.weight, (self.W_beta+self.W_alpha)/(2.0**(self.W_precision.get_bits())-1), self.W_alpha, self.W_beta)
             else:
                 W_quant = pact_quantize_signed(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
-        elif self.quantize_W and not self.deployment:
-            if self.quant_asymm:
-                W_quant = pact_quantize_asymm_inference(self.weight, (self.W_beta+self.W_alpha)/(2.0**(self.W_precision.get_bits())-1), self.W_alpha, self.W_beta, train_loop=self.train_loop)
+        if self.training and self.quantize_b and not self.deployment:
+            if self.quant_asymm_b:
+                b_quant = pact_quantize_asymm(self.bias, (self.b_beta + self.b_alpha) / (2.0 ** (self.b_precision.get_bits()) - 1), self.b_alpha, self.b_beta)
             else:
-                W_quant = pact_quantize_signed_inference(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
+                b_quant = pact_quantize_signed(self.bias, 2 * self.b_alpha / (2.0 ** (self.b_precision.get_bits()) - 1), self.b_alpha)
+        
+        elif (self.quantize_W or self.quantize_b) and not self.deployment:
+            if self.quantize_W:
+                if self.quant_asymm:
+                    W_quant = pact_quantize_asymm_inference(self.weight, (self.W_beta+self.W_alpha)/(2.0**(self.W_precision.get_bits())-1), self.W_alpha, self.W_beta, train_loop=self.train_loop)
+                else:
+                    W_quant = pact_quantize_signed_inference(self.weight, 2*self.W_alpha/(2.0**(self.W_precision.get_bits())-1), self.W_alpha)
+            if self.quantize_b:
+                if self.quant_asymm_b:
+                    b_quant = pact_quantize_asymm_inference_bias(self.bias, (self.b_beta+self.b_alpha)/(2.0**(self.b_precision.get_bits())-1), self.b_alpha, self.b_beta, train_loop=self.train_loop)
+                else:
+                    b_quant = pact_quantize_signed_inference_bias(self.bias, 2*self.b_alpha/(2.0**(self.b_precision.get_bits())-1), self.b_alpha)
+                
         else:
             W_quant = self.weight
-        y = torch.nn.functional.linear(x_quant, W_quant, self.bias)
+            b_quant = self.bias
+     
+        y = torch.nn.functional.linear(x_quant, W_quant, b_quant)
         if not self.training and self.quantize_W:
             del W_quant
         # y is returned non-quantized, as it is assumed to be quantized after BN
diff --git a/transf/deploy.py b/transf/deploy.py
index 2b1dcec..34d4010 100644
--- a/transf/deploy.py
+++ b/transf/deploy.py
@@ -65,6 +65,38 @@ def _round_weights_pact(self, **kwargs):
             m.kappa.data[:] += m.eps_kappa/2
             m.lamda.data[:] += m.eps_lamda/2
 
+def _harden_bias_pact(self, **kwargs):
+    r"""Harden all bias in the network to their quantized value.
+
+    """
+
+    for n,m in self.named_modules():
+        if (m.__class__.__name__ == "PACT_Conv2d" or \
+            m.__class__.__name__ == "PACT_Conv1d" or \
+            m.__class__.__name__ == "PACT_Linear"):
+            if(m.bias is not None):
+                m.train_loop_oldprec_b = float(m.b_beta.item()+m.b_alpha.item())/(2.0**(m.b_precision.get_bits())-1)
+                m.harden_bias(**kwargs)
+        if (m.__class__.__name__ == "PACT_QuantizedBatchNormNd"):
+            m.harden_bias(**kwargs)
+
+def _round_bias_pact(self, **kwargs):
+    r"""Round all bias in the network adding 1/2 an eps.
+
+    """
+
+    for n,m in self.named_modules():
+        if (m.__class__.__name__ == "PACT_Conv2d" or \
+            m.__class__.__name__ == "PACT_Conv1d" or \
+            m.__class__.__name__ == "PACT_Linear"):
+             if(m.bias is not None):
+                m.bias.data[:] += (m.b_beta.item()+m.b_alpha.item())/(2.0**(m.b_precision.get_bits())-1) / 2
+        #if (m.__class__.__name__ == "PACT_QuantizedBatchNormNd"):
+        #    m.kappa.data[:] += m.eps_kappa/2
+        #    m.lamda.data[:] += m.eps_lamda/2
+
+
+
 def _set_deployment_pact(self, eps_in, only_activations=False, **kwargs):
     r"""Sets the network in deployment mode, enabling saving it to ONNX format or similar.
 
@@ -133,7 +165,9 @@ def _qd_stage(self, eps_in, add_input_bias_dict={}, remove_bias_dict={}, prune_e
     if prune_empty_bn:
         self.prune_empty_bn(threshold=1e-9)
     self.round_weights()
+    self.round_bias() 
     self.harden_weights()
+    self.harden_bias()
     if add_input_bias_dict:
         self.add_input_bias(add_input_bias_dict)
     if remove_bias_dict:
@@ -153,7 +187,8 @@ def _qd_stage(self, eps_in, add_input_bias_dict={}, remove_bias_dict={}, prune_e
         self.calibrate_bn(minmax=False, range_factor=bn_calibration_range_factor, **kwargs)
     self.set_deployment(eps_in=eps_in, **kwargs) # repeat, to fix BN eps
     self.harden_weights()
-
+    self.harden_bias()
+    
 def _id_stage(self, eps_in=None, **kwargs):
     r"""High-level function to move the network from QD to ID stage.
 
diff --git a/transf/export.py b/transf/export.py
index 739ce7b..d5fbbdc 100644
--- a/transf/export.py
+++ b/transf/export.py
@@ -51,6 +51,12 @@ def _export_precision(self):
                 d[n]['W_scale'] = m.W_precision.get_scale()
         except AttributeError:
             pass
+        try:
+            d[n]['b_bits']  = m.b_precision.get_bits()
+            if not "PACT" in m.__class__.__name__:
+                d[n]['b_scale'] = m.b_precision.get_scale()
+        except AttributeError:
+            pass
         if len(d[n].keys()) == 0 or n == "":
             d.pop(n, None)
     return d
diff --git a/transf/utils.py b/transf/utils.py
index 0b0a26f..73cf909 100644
--- a/transf/utils.py
+++ b/transf/utils.py
@@ -1,6 +1,7 @@
 #
 # utils.py
 # Francesco Conti <fconti@iis.ee.ethz.ch>
+# Alfio Di Mauro <adimauro@iis.ee.ethz.ch>
 #
 # Copyright (C) 2018-2020 ETH Zurich
 # 
@@ -17,139 +18,293 @@
 # limitations under the License.
 
 import torch
-import nemo
-from nemo.precision import Precision
-from nemo.quant.pact import *
-from nemo.graph import DeployGraph
-from torch.nn.modules.utils import _single,_pair
-from collections import OrderedDict
-import types
 import logging
-import numpy as np
-import copy
-import math
-import torchvision.models
+import os
+import json
 import re
-from nemo.transf.common import *
-
-def _change_precision_pact(self, bits=4, scale_activations=True, scale_weights=True, verbose=True, reset_alpha=True, min_prec_dict=None, **kwargs):
-    r"""Changes the target precision of a PACT quantization-aware layer.
+import nemo
+import numpy as np
+from collections import OrderedDict
 
-    
-    :param bits: target bit-width.
-    :type  bits: `int`
+def precision_dict_to_json(d, filename=None):
+    s = json.dumps(d, indent=4)
+    if filename is not None:
+        with open(filename, 'w') as f:
+            f.write(s)
+    else:
+        return s
 
-    :param scale_activations: if False, do not change precision of activations (default True).
-    :type  scale_activations: boolean
+def precision_dict_from_json(filename):
+    with open(filename, "r") as f:
+        rr = json.load(f)
+    return rr
 
-    :param scale_weights: if False, do not change precision of weights (default True).
-    :type  scale_weights: boolean
+def process_json(json, args):
+    args = vars(args)
+    regime = {}
+    if args.regime is not None:
+        with open(args.regime, "r") as f:
+            rr = json.load(f)
+        for k in rr.keys():
+            try:
+                regime[int(k)] = rr[k]
+            except ValueError:
+                regime[k] = rr[k]
 
-    :param verbose: if False, do not log precision information (default True).
-    :type  verbose: boolean
+def save_checkpoint(net, optimizer, epoch, acc=0.0, checkpoint_name='net_', checkpoint_suffix=''):
+    checkpoint_name = checkpoint_name + checkpoint_suffix
+    logging.info('Saving checkpoint %s' % checkpoint_name)
+    try:
+        optimizer_state = optimizer.state_dict()
+    except AttributeError:
+        optimizer_state = None
+    try:
+        precision = net.export_precision()
+    except AttributeError:
+        precision = None
+    state = {
+        'epoch': epoch + 1,
+        'state_dict': net.state_dict(),
+        'precision': precision,
+        'acc': acc,
+        'optimizer' : optimizer_state,
+    }
+    if not os.path.isdir('checkpoint'):
+        os.mkdir('checkpoint')
+    torch.save(state, './checkpoint/%s.pth' % (checkpoint_name))
 
-    :param reset_alpha: if False, do not reset weight scale parameter upon precision change (default True).
-    :type  reset_alpha: boolean
+def export_onnx(file_name, net, net_inner, input_shape, device, round_params=True, perm=None, redefine_names=False, batch_size=1, verbose=False):
+    if perm is None:
+        perm = lambda x : x
+    pattern = re.compile('[\W_]+')
+    if (device is not None):
+        dummy_input = perm(torch.randn(batch_size, *input_shape, device=device))
+    else:
+        dummy_input = perm(torch.randn(batch_size, *input_shape, device='cuda' if torch.cuda.is_available() else 'cpu'))
+    net.eval()
+    # rounding of parameters to avoid strange numerical errors on writeout
+    if round_params:
+        with torch.no_grad():
+            for param in net_inner.parameters(recurse=True):
+                if param.dtype is torch.float32:
+                    param[:] = torch.round(param)
+    if redefine_names:
+        input_names  = [ 'input' ] + [ pattern.sub('_', n) for n,_ in net_inner.named_parameters() ]
+        output_names = [ 'output' ]
+        torch.onnx.export(net_inner, dummy_input, file_name, verbose=verbose, do_constant_folding=True, input_names=input_names, output_names=output_names, export_params=True)
+    else:
+        torch.onnx.export(net_inner, dummy_input, file_name, verbose=verbose, do_constant_folding=True, export_params=True)
 
-    :param min_prec_dict: dictionary of minimum layer-by-layer precisions (default None).
-    :type  min_prec_dict: dictionary
+PRECISION_RULE_KEYS_REQUIRED = {
+    "for_epochs": 1,
+    "for_epochs_no_abs_bound": 3,
+    "delta_loss_less_than": 0.01,
+    "running_avg_memory": 5,
+    "delta_loss_running_std_stale": 1.5,
+    "abs_loss_stale": 1.4,
+    "scale_lr": True,
+    "lr_scaler": 4.0,
+    "divergence_abs_threshold": 1e9
+}
+PRECISION_RULE_KEYS_ALLOWED = [
+    "custom_scaler",
+    "bit_scaler",
+    "bit_stop_condition"
+]
+ 
+def parse_precision_rule(rule):
+    required = list(PRECISION_RULE_KEYS_REQUIRED.keys())
+    allowed  = PRECISION_RULE_KEYS_ALLOWED + required
+    for k in required:
+        if not k in rule:
+            rule[k] = PRECISION_RULE_KEYS_REQUIRED[k]
+    flag = False
+    for k in rule.keys():
+        if not k in allowed and not k.isdigit():
+            print("[ERROR] %s is not a key allowed in the relaxation rule", k)
+            flag = True
+    if "bit_scaler" in rule and not "W_bit_scaler" in rule:
+        rule["W_bit_scaler"] = rule["bit_scaler"]
+    if "bit_scaler" in rule and not "x_bit_scaler" in rule:
+        rule["x_bit_scaler"] = rule["bit_scaler"]
+    if "bit_stop_condition" in rule and not "W_bit_stop_condition" in rule:
+        rule["W_bit_stop_condition"] = rule["bit_stop_condition"]
+    if "bit_stop_condition" in rule and not "x_bit_stop_condition" in rule:
+        rule["x_bit_stop_condition"] = rule["bit_stop_condition"]
+    if flag:
+        import sys; sys.exit(1)
+    print(list(rule.keys()))
+    return rule
+    
+# see https://github.com/sksq96/pytorch-summary
+def get_summary(net, input_size, batch_size=1, device="cuda", verbose=False):
+    s = ""
+    mdict = {}
+    for n,m in net.named_modules():
+        mdict[n] = m
+    def register_hook(module):
+        def hook(module, input, output):
+            class_name = str(module.__class__).split(".")[-1].split("'")[0]
+            module_idx = len(summary)
+            m_key = next(n for n,m in mdict.items() if m==module)
+            summary[m_key] = OrderedDict()
+            summary[m_key]["input_shape"] = list(input[0].size())
+            summary[m_key]["input_shape"][0] = batch_size
+            if isinstance(output, (list, tuple)):
+                summary[m_key]["output_shape"] = [
+                    [-1] + list(o.size())[1:] for o in output
+                ]
+            else:
+                summary[m_key]["output_shape"] = list(output.size())
+                summary[m_key]["output_shape"][0] = batch_size
 
-    """
-    if scale_activations and bits is not None:
-        self.x_precision.bits = bits
-    if scale_weights and bits is not None:
-        self.W_precision.bits = bits
-    for n,m in self.named_modules():
-        min_prec_x = copy.deepcopy(self.x_precision)
-        min_prec_W = copy.deepcopy(self.W_precision)
-        if min_prec_dict is not None:
-            try:
-                min_prec_x.bits = min_prec_dict[n]['x_bits']
-            except KeyError:
-                pass
-            try:
-                min_prec_W.bits = min_prec_dict[n]['W_bits']
-            except KeyError:
-                pass
-        if m.__class__.__name__ == "PACT_Act" and scale_activations:
-            m.precision = max(self.x_precision, min_prec_x)
-        if scale_weights and (m.__class__.__name__ == "PACT_Conv2d" or \
-                              m.__class__.__name__ == "PACT_Conv1d" or \
-                              m.__class__.__name__ == "PACT_Linear"):
-            m.W_precision = max(self.W_precision, min_prec_W)
-            if reset_alpha:
-                m.reset_alpha_weights()
-        if verbose and (m.__class__.__name__ == "PACT_Act") and scale_activations:
-            try:
-                logging.info("[Quant]\t\t %s: x_bits=%.2f" % (n, m.precision.get_bits()))
-            except AttributeError:
-                pass
-        if verbose and scale_weights and (m.__class__.__name__ == "PACT_Conv2d" or \
-                                          m.__class__.__name__ == "PACT_Conv1d" or \
-                                          m.__class__.__name__ == "PACT_Linear"):
-            try:
-                logging.info("[Quant]\t\t %s: W_bits=%.2f" % (n, m.W_precision.get_bits()))
-            except AttributeError:
-                pass
+            params = 0
+            if hasattr(module, "weight") and hasattr(module.weight, "size"):
+                try:
+                    params += torch.prod(torch.LongTensor(list(module.weight.size()))) / module.group
+                except AttributeError:
+                    params += torch.prod(torch.LongTensor(list(module.weight.size())))
+                summary[m_key]["trainable"] = module.weight.requires_grad
+            if hasattr(module, "bias") and hasattr(module.bias, "size"):
+                params += torch.prod(torch.LongTensor(list(module.bias.size())))
+            summary[m_key]["nb_params"] = params
+            
+            if hasattr(module, "W_precision"):
+                summary[m_key]['W_bits'] = module.W_precision.get_bits()
+            
+            if hasattr(module, "precision"):
+                summary[m_key]['bits'] = module.precision.get_bits()
 
-def _set_train_loop_pact(self):
-    r"""Sets modules so that weights are not treated like hardened (e.g., for training).
-    
-    """
+        if (
+            not isinstance(module, torch.nn.Sequential)
+            and not isinstance(module, torch.nn.ModuleList)
+            and not (module == net)
+        ):
+            hooks.append(module.register_forward_hook(hook))
 
-    for n,m in self.named_modules():
-        if (m.__class__.__name__ == "PACT_Conv2d" or \
-            m.__class__.__name__ == "PACT_Conv1d" or \
-            m.__class__.__name__ == "PACT_Linear" ):
-            m.train_loop = True
+    device = device.lower()
+    assert device in [
+        "cuda",
+        "cpu",
+    ], "Input device is not valid, please specify 'cuda' or 'cpu'"
 
-def _unset_train_loop_pact(self):
-    r"""Sets modules so that weights are treated like hardened (e.g., for evaluation).
-    
-    """
+    if device == "cuda" and torch.cuda.is_available():
+        dtype = torch.cuda.FloatTensor
+    else:
+        dtype = torch.FloatTensor
 
-    for n,m in self.named_modules():
-        if (m.__class__.__name__ == "PACT_Conv2d" or \
-            m.__class__.__name__ == "PACT_Conv1d" or \
-            m.__class__.__name__ == "PACT_Linear" ):
-            m.train_loop = False
-            m.train_loop_oldprec = float(m.W_beta.item()+m.W_alpha.item())/(2.0**(m.W_precision.get_bits())-1)
+    # multiple inputs to the network
+    if isinstance(input_size, tuple):
+        input_size = [input_size]
 
-def _reset_alpha_act_pact(self, **kwargs):
-    r"""Resets :py:class:`nemo.quant.PACT_Act` parameter `alpha` the value collected through statistics.
-    
-    """
+    # batch_size of 2 for batchnorm
+    x = [torch.rand(2, *in_size).type(dtype) for in_size in input_size]
 
-    for n,m in self.named_modules():
-        if m.__class__.__name__ == "PACT_Act":
-            m.reset_alpha(**kwargs)
+    # create properties
+    summary = OrderedDict()
+    hooks = []
 
-def _get_nonclip_parameters_pact(self):
-    r"""Yields all parameters except for `alpha` values.
-    
-    """
+    # register hook
+    net.apply(register_hook)
 
-    for name, param in self.named_parameters(recurse=True):
-        if name[-5:] != 'alpha':
-            yield param
+    # make a forward pass
+    net(*x)
 
-def _get_clip_parameters_pact(self):
-    r"""Yields all `alpha` parameters.
-    
-    """
+    # remove these hooks
+    for h in hooks:
+        h.remove()
 
-    for name, param in self.named_parameters(recurse=True):
-        if name[-5:] == 'alpha':
-            yield param
+    s += "----------------------------------------------------------------" + "\n"
+    line_new = "{:>20}  {:>25} {:>15}".format("Layer (type)", "Output Shape", "Param #")
+    s += line_new + "\n"
+    s += "================================================================" + "\n"
+    total_params = 0
+    total_output = 0
+    trainable_params = 0
+    params_size = 0
+    output_size = 0
+    input_size = 0
+    for layer in summary:
+        # input_shape, output_shape, trainable, nb_params
+        line_new = "{:>20}  {:>25} {:>15}".format(
+            layer,
+            str(summary[layer]["output_shape"]),
+            "{0:,}".format(summary[layer]["nb_params"]),
+        )
+        total_params += summary[layer]["nb_params"]
+        try:
+            params_size += abs(summary[layer]["nb_params"]  * summary[layer]["W_bits"] / 8. / (1024.))
+        except KeyError:
+            params_size += abs(summary[layer]["nb_params"] * 32. / 8. / (1024.))
+        total_output += np.prod(summary[layer]["output_shape"])
+        try:
+            output_size = max(output_size, np.prod(summary[layer]["output_shape"]) * summary[layer]["bits"] / 8 / (1024.))
+        except KeyError:
+            output_size = max(output_size, np.prod(summary[layer]["output_shape"]) * 32 / 8 / (1024.))
+        if "trainable" in summary[layer]:
+            if summary[layer]["trainable"] == True:
+                trainable_params += summary[layer]["nb_params"]
+        s += line_new + "\n"
 
-def _reset_alpha_weights_pact(self, method='standard', **kwargs):
-    r"""Resets parameter `W_alpha`.
-    
-    """
+    s += "================================================================" + "\n"
+    s += "Total params: {0:,}".format(total_params) + "\n"
+    s += "Trainable params: {0:,}".format(trainable_params) + "\n"
+    s += "Non-trainable params: {0:,}".format(total_params - trainable_params) + "\n"
+    s += "----------------------------------------------------------------" + "\n"
+    s += "Biggest activation tensor size (kB): {0:,.2f}".format(output_size) + "\n"
+    s += "Params size (kB): {0:,.1f}".format(params_size) + "\n"
+    s += "----------------------------------------------------------------" + "\n"
+    if verbose:
+        logging.info(s)
+    return { 'dict': summary, 'prettyprint': s, 'biggest_activation': output_size, 'params_size': params_size }
 
-    for n,m in self.named_modules():
-        if (m.__class__.__name__ == "PACT_Conv2d" or \
-            m.__class__.__name__ == "PACT_Conv1d" or \
-            m.__class__.__name__ == "PACT_Linear"):
-            m.reset_alpha_weights(**kwargs)
+def get_intermediate_activations(net, test_fn, *test_args, **test_kwargs):
+    l = len(list(net.named_modules()))
+    buffer_in  = OrderedDict([])
+    buffer_out = OrderedDict([])
+    hooks = OrderedDict([])
+    def get_hk(n):
+        def hk(module, input, output):
+            buffer_in  [n] = input
+            buffer_out [n] = output
+        return hk
+    for i,(n,l) in enumerate(net.named_modules()):
+        hk = get_hk(n)
+        hooks[n] = l.register_forward_hook(hk)
+    ret = test_fn(*test_args, **test_kwargs)
+    for n,l in net.named_modules():
+        hooks[n].remove()
+    return buffer_in, buffer_out, ret
+        
+def get_intermediate_eps(net, eps_in):
+    l = len(list(net.named_modules()))
+    eps = OrderedDict([])
+    for i,(n,l) in enumerate(net.named_modules()):
+        eps[n] = net.get_eps_at(n, eps_in)
+    return eps
 
+def get_integer_activations(buf, eps, net=None):
+    if type(eps) is float and net is None:
+        return buf
+    elif type(eps) is float:
+        eps_in = eps
+        eps = OrderedDict([])
+        for n,m in net.named_modules():
+            try:
+                eps[n] = m.get_output_eps(eps_in)
+            except AttributeError:
+                pass
+    buf_ = OrderedDict([])
+    for n in buf.keys():
+        b = buf.get(n, None)
+        e = eps.get(n, None)
+        if b is None or e is None:
+            continue
+        if type(buf[n]) is tuple or type(buf[n]) is list:
+            buf_[n] = []
+            for b in buf[n]:
+                buf_[n].append((b / eps[n]).floor()) # FIXME
+        else:
+            buf_[n] = (buf[n] / eps[n]).floor()
+    return buf_
+    
diff --git a/transform.py b/transform.py
index 616abbb..70588f0 100644
--- a/transform.py
+++ b/transform.py
@@ -42,7 +42,7 @@ from nemo.transf.statistics import *
 from nemo.transf.utils import *
 from nemo.transf.sawb import *
 
-def quantize_pact(module, W_bits=4, x_bits=4, dummy_input=None, remove_dropout=False, **kwargs):
+def quantize_pact(module, W_bits=4, x_bits=4, b_bits=7, dummy_input=None, remove_dropout=False, **kwargs):
     r"""Takes a PyTorch module and makes it quantization-aware with PACT, recursively.
 
     The function follows recursively the data structures containing PyTorch layers (typically as hierarchical lists, e.g.
@@ -116,6 +116,8 @@ def quantize_pact(module, W_bits=4, x_bits=4, dummy_input=None, remove_dropout=F
     module.set_eps_in                  = types.MethodType(nemo.transf.deploy._set_eps_in_pact, module)
     module.round_weights               = types.MethodType(nemo.transf.deploy._round_weights_pact, module)
     module.harden_weights              = types.MethodType(nemo.transf.deploy._harden_weights_pact, module)
+    module.round_bias                  = types.MethodType(nemo.transf.deploy._round_bias_pact, module)
+    module.harden_bias                 = types.MethodType(nemo.transf.deploy._harden_bias_pact, module)
     module.set_deployment              = types.MethodType(nemo.transf.deploy._set_deployment_pact, module)
     module.qd_stage                    = types.MethodType(nemo.transf.deploy._qd_stage, module)
     module.id_stage                    = types.MethodType(nemo.transf.deploy._id_stage, module)
@@ -123,6 +125,7 @@ def quantize_pact(module, W_bits=4, x_bits=4, dummy_input=None, remove_dropout=F
     module.export_weights_legacy_int16 = types.MethodType(nemo.transf.export._export_weights_legacy_int16, module)
     module.change_precision            = types.MethodType(nemo.transf.utils._change_precision_pact, module)
     module.reset_alpha_weights         = types.MethodType(nemo.transf.utils._reset_alpha_weights_pact, module)
+    module.reset_alpha_bias            = types.MethodType(nemo.transf.utils._reset_alpha_bias_pact, module)
     module.reset_alpha_act             = types.MethodType(nemo.transf.utils._reset_alpha_act_pact, module)
     module.get_clip_parameters         = types.MethodType(nemo.transf.utils._get_clip_parameters_pact, module)
     module.get_nonclip_parameters      = types.MethodType(nemo.transf.utils._get_nonclip_parameters_pact, module)
@@ -142,6 +145,7 @@ def quantize_pact(module, W_bits=4, x_bits=4, dummy_input=None, remove_dropout=F
     module.disable_grad_sawb           = types.MethodType(nemo.transf.sawb._disable_grad_sawb, module)
     module.weight_clip_sawb            = types.MethodType(nemo.transf.sawb._weight_clip_sawb, module)
     module.W_precision = Precision(W_bits, None)
+    module.b_precision = Precision(b_bits, None)
     module.x_precision = Precision(x_bits, None)
     return module
 
@@ -203,6 +207,8 @@ def _hier_integerizer(module, **kwargs):
         module.__class__.__name__ == "PACT_Conv1d" or \
         module.__class__.__name__ == "PACT_Linear"):
         module.integerize_weights(**kwargs)
+        if(module.bias is not None):
+            module.integerize_bias(**kwargs)
         return module
     elif (module.__class__.__name__ == "PACT_QuantizedBatchNormNd"):
         module = PACT_IntegerBatchNormNd(kappa=module.kappa, lamda=module.lamda, eps_in=module.eps_in, eps_kappa=module.eps_kappa, eps_lamda=module.eps_lamda)
diff --git a/utils.py b/utils.py
index 2d24297..73cf909 100644
--- a/utils.py
+++ b/utils.py
@@ -73,17 +73,21 @@ def save_checkpoint(net, optimizer, epoch, acc=0.0, checkpoint_name='net_', chec
         os.mkdir('checkpoint')
     torch.save(state, './checkpoint/%s.pth' % (checkpoint_name))
 
-def export_onnx(file_name, net, net_inner, input_shape, round_params=True, perm=None, redefine_names=False, batch_size=1, verbose=False):
+def export_onnx(file_name, net, net_inner, input_shape, device, round_params=True, perm=None, redefine_names=False, batch_size=1, verbose=False):
     if perm is None:
         perm = lambda x : x
     pattern = re.compile('[\W_]+')
-    dummy_input = perm(torch.randn(batch_size, *input_shape, device='cuda' if torch.cuda.is_available() else 'cpu'))
+    if (device is not None):
+        dummy_input = perm(torch.randn(batch_size, *input_shape, device=device))
+    else:
+        dummy_input = perm(torch.randn(batch_size, *input_shape, device='cuda' if torch.cuda.is_available() else 'cpu'))
     net.eval()
     # rounding of parameters to avoid strange numerical errors on writeout
     if round_params:
-        for param in net_inner.parameters(recurse=True):
-            if param.dtype is torch.float32:
-                param[:] = torch.round(param)
+        with torch.no_grad():
+            for param in net_inner.parameters(recurse=True):
+                if param.dtype is torch.float32:
+                    param[:] = torch.round(param)
     if redefine_names:
         input_names  = [ 'input' ] + [ pattern.sub('_', n) for n,_ in net_inner.named_parameters() ]
         output_names = [ 'output' ]
